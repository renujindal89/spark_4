{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9206e02-bc23-450e-8491-d4145343573b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# INTRODUCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4457ec29-d486-4f3e-9206-5e6cb525deea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "A Catalog is the top-level container for organizing and managing data assets in Databricks.\n",
    "Catalog\n",
    " └── Schema (Database)\n",
    "      └── Tables / Views / Volumes\n",
    "learning\n",
    " └── learning\n",
    "      ├── employees (table)\n",
    "      ├── employees_parquet_single (volume)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3c4babd-b760-422b-855c-cb28b125da01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Schema\n",
    "1.Logical database inside a catalog\n",
    "2.Schema = Database (same thing)\n",
    "Volume\n",
    "A Volume is a governed storage location in Unity Catalog used to store files (not tables)\n",
    "It is Databricks’ managed replacement for DBFS when using Unity Catalog.\n",
    "We can store :\n",
    "Parquet files\n",
    "JSON / CSV files\n",
    "Images\n",
    "\n",
    "Hierarchy\n",
    "Catalog\n",
    " └── Schema\n",
    "      └── Volume\n",
    "           └── Files (parquet, json, csv, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56c095d0-4307-46ac-8b28-1c26042dae70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Volume vs Table (Very important)\n",
    "Feature            \tVolume\t                       Table\n",
    "Stores\t              Files\t                        Structured data\n",
    "Data format\t             Any\t                      Parquet\n",
    "Queryable by SQL\t     ❌                      \t✅\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df00fbb7-8452-4042-968d-3d0830c93e40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3156ba03-a928-43aa-9b4c-361edac11ff9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>catalog</th></tr></thead><tbody><tr><td>demo1</td></tr><tr><td>learning</td></tr><tr><td>learning2</td></tr><tr><td>learning3</td></tr><tr><td>samples</td></tr><tr><td>spark4</td></tr><tr><td>spark_4</td></tr><tr><td>system</td></tr><tr><td>workspace</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "demo1"
        ],
        [
         "learning"
        ],
        [
         "learning2"
        ],
        [
         "learning3"
        ],
        [
         "samples"
        ],
        [
         "spark4"
        ],
        [
         "spark_4"
        ],
        [
         "system"
        ],
        [
         "workspace"
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "catalog",
            "nullable": false,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 2
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "catalog",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SHOW CATALOGS;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5068ad4-bcea-4bd7-9667-2b85de02d3c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>databaseName</th></tr></thead><tbody><tr><td>default</td></tr><tr><td>file_handling</td></tr><tr><td>information_schema</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "default"
        ],
        [
         "file_handling"
        ],
        [
         "information_schema"
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "databaseName",
            "nullable": false,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 3
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "databaseName",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SHOW SCHEMAS IN spark_4;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8faa1f9c-183b-4e4e-ab5b-b6db3a88e01b",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1765818577653}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>database</th><th>volume_name</th></tr></thead><tbody><tr><td>learning</td><td>learning</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "learning",
         "learning"
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "database",
            "nullable": false,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "volume_name",
            "nullable": false,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 56
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "database",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "volume_name",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SHOW VOLUMES IN learning.learning;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b11f89ec-d1c8-4c29-bfb2-f029581e7b73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# MODULE_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eac3e708-34f5-46ac-a3f5-efbb933db7e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# spark.read\n",
    "#df.write\n",
    "#.format(\"csv\" / \"json\" / \"parquet\")\n",
    "#.option(\"key\", \"value\")\n",
    "#.load(\"path\") / .save(\"path\")\n",
    "#eg.df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(\"/Workspace/Users/renugoel89@gmail.com/Drafts/samples_data.csv\")\n",
    "#eg.  df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"/Workspace/Users/renugoel89@gmail.com/Drafts/samples_data.csv\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff255452-2805-409f-9fb5-620dc4d35c6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# write mode\n",
    "df.write.format(\"csv\").mode(\"overwrite\").save(\"path/students\")\n",
    "df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"/Volumes/learning/learning/learning/students.csv\")\n",
    "#df.write → starts the writer.\n",
    "#.format(\"json\") → saves in JSON format.\n",
    "#.mode(\"overwrite\") → replace existing data if present.\n",
    "#.save(\"path\") → actually writes to given location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f034d0a-a087-4b7b-9ecb-d1ce781865cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    ".mode()\n",
    "Defines what to do if output path already exists.\n",
    "Options:\n",
    "overwrite = Replace old notebook with a new one.\n",
    "append = Add extra pages to an existing notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7758f7a-c20e-4de5-bded-8a998599bfc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    ".option(\"key\", \"value\")\n",
    "Used to provide extra settings while reading or writing.\n",
    "Common examples:\n",
    ".option(\"header\", \"true\") → use first row as column names.\n",
    ".option(\"inferSchema\", \"true\") → automatically detect column data types.\n",
    ".option(\"sep\", \"\\t\") → set column delimiter (e.g., tab).\n",
    ".option(\"multiline\", \"true\") → read multi-line JSON.\n",
    "Options = customize how Spark handles the file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48b7f2b2-9c5e-4af3-a3c7-c4b019b0cf29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    ".save(\"path\")\n",
    "Writes the DataFrame into the folder output/students.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55d01b62-44ac-4595-bf38-783ea83fbd7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# CSV FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "782da2c6-d1b7-44d5-a203-96ae7fcb71b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(\"/Volumes/learning/learning/learning/samples_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84e704db-66e6-43cb-8258-147f25c94765",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+-------+-------+----------+----------+-----+\n| id|   name|age| gender| salary| join_date|department|score|\n+---+-------+---+-------+-------+----------+----------+-----+\n|  1|  Alice| 25|      F|  50000|10-05-2021|     Sales|   88|\n|  2|    Bob| 30|      M|  62000|15-03-2020|        HR|   92|\n|  3|Charlie| 30|      M|  58000|15-07-2019|        IT|   79|\n|  4|  david| 45|   male|  45000|        IT|        IT|   45|\n|  5|    Eve| -3|      F|  72000|2022-13-01|        IT|   65|\n|  5|    Eve|200|      F|1000000|20-11-2018|   Finance|  300|\n|  7|  Frank| 33|      M|  54000|wrong_date|   finance|   73|\n|  8|   Renu| 29| FEMALE|  51000|01-09-2021|     Sales|   85|\n|  9| Grace | 30|      F|  49000|01-12-2020|     SALES|   90|\n| 10|  Henry| 41|Unknown|  34000|05-05-2017|        IT|   82|\n+---+-------+---+-------+-------+----------+----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e19cdc55-e227-456c-b6ff-7a88ac403d63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"/Volumes/learning/learning/learning/samples_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "740fd59c-ad9c-49c6-b2a7-a38aca07f33b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+-------+-------+----------+----------+-----+\n| id|   name|age| gender| salary| join_date|department|score|\n+---+-------+---+-------+-------+----------+----------+-----+\n|  1|  Alice| 25|      F|  50000|10-05-2021|     Sales|   88|\n|  2|    Bob| 30|      M|  62000|15-03-2020|        HR|   92|\n|  3|Charlie| 30|      M|  58000|15-07-2019|        IT|   79|\n|  4|  david| 45|   male|  45000|        IT|        IT|   45|\n|  5|    Eve| -3|      F|  72000|2022-13-01|        IT|   65|\n|  5|    Eve|200|      F|1000000|20-11-2018|   Finance|  300|\n|  7|  Frank| 33|      M|  54000|wrong_date|   finance|   73|\n|  8|   Renu| 29| FEMALE|  51000|01-09-2021|     Sales|   85|\n|  9| Grace | 30|      F|  49000|01-12-2020|     SALES|   90|\n| 10|  Henry| 41|Unknown|  34000|05-05-2017|        IT|   82|\n+---+-------+---+-------+-------+----------+----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7be950c2-2a4d-41e6-a5a9-429a6ba2d92c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#JSON FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ad2fad2-366c-4340-aa7a-ea6eb3713b60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .json(\"/Volumes/learning/learning/learning/employees.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a751b58a-d920-4284-b5f5-6383766a2e29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# error means\n",
    "This output means Spark could not parse your data correctly\n",
    "What does _corrupt_record mean?\n",
    "Spark tried to read a file (CSV / JSON)\n",
    "Schema inference failed\n",
    "The entire row could not be parsed\n",
    "\n",
    "This usually happens when:\n",
    "Wrong file format\n",
    "Incorrect delimiter\n",
    "Multiline JSON without multiline=true\n",
    "Header mismatch\n",
    "Invalid data\n",
    "\n",
    "Solution can be\n",
    "Provide Schema Explicitly (Best Practice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9c59b0e-8150-43bd-88d9-015a2a6b3c63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"department\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7d2fc26-6cab-48fc-b4bf-8cfe34e17f87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = spark.read \\\n",
    "    .schema(schema) \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .json(\"/Volumes/learning/learning/learning/employees.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2e69c5b-d607-401c-a4be-8af46834ee58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+------+\n| id|   name|department|salary|\n+---+-------+----------+------+\n|  1|  Alice|        IT| 75000|\n|  2|    Bob|        HR| 65000|\n|  3|Charlie|   Finance| 80000|\n+---+-------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d50b79dc-1dcf-4adc-8b9a-b65d2fd6f811",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+-------+-------+----------+----------+-----+\n| id|   name|age| gender| salary| join_date|department|score|\n+---+-------+---+-------+-------+----------+----------+-----+\n|  1|  Alice| 25|      F|  50000|10-05-2021|     Sales|   88|\n|  2|    Bob| 30|      M|  62000|15-03-2020|        HR|   92|\n|  3|Charlie| 30|      M|  58000|15-07-2019|        IT|   79|\n|  4|  david| 45|   male|  45000|        IT|        IT|   45|\n|  5|    Eve| -3|      F|  72000|2022-13-01|        IT|   65|\n|  5|    Eve|200|      F|1000000|20-11-2018|   Finance|  300|\n|  7|  Frank| 33|      M|  54000|wrong_date|   finance|   73|\n|  8|   Renu| 29| FEMALE|  51000|01-09-2021|     Sales|   85|\n|  9| Grace | 30|      F|  49000|01-12-2020|     SALES|   90|\n| 10|  Henry| 41|Unknown|  34000|05-05-2017|        IT|   82|\n+---+-------+---+-------+-------+----------+----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e4d9b34-e89a-49a7-a20b-f67adcf69b70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# FILTER THE DATA and write in other file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d2815c3-45e8-4b19-9604-0602ca83ddc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+-------+------+----------+----------+-----+\n| id|   name|age| gender|salary| join_date|department|score|\n+---+-------+---+-------+------+----------+----------+-----+\n|  3|Charlie| 30|      M| 58000|15-07-2019|        IT|   79|\n|  4|  david| 45|   male| 45000|        IT|        IT|   45|\n|  5|    Eve| -3|      F| 72000|2022-13-01|        IT|   65|\n| 10|  Henry| 41|Unknown| 34000|05-05-2017|        IT|   82|\n+---+-------+---+-------+------+----------+----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lower, trim\n",
    "\n",
    "it_df = df.filter(\n",
    "    lower(trim(col(\"department\"))) == \"it\"\n",
    ")\n",
    "\n",
    "it_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a53d80a-1e79-40f0-ab45-c34c51be9a2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# WRITE MODE CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b60249c4-8b5e-41ef-aea7-9a734146d50f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "it_df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"/Volumes/learning/learning/learning/it_df.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8047933-9d67-41b2-bea0-356e7488b88f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# LETS CHECK  create only one file  \n",
    "Spark can write multiple CSV files, but it depends on the number of partitions in it_df\n",
    "✅ Reason: it_df has only ONE partition\n",
    "#Common reasons why it_df has 1 partition\n",
    "1️⃣ Small dataset\n",
    "Spark automatically uses 1 partition for small data.\n",
    "\n",
    "2️⃣ DataFrame created manually\n",
    "\n",
    "If you did:\n",
    "it_df = spark.createDataFrame(data)\n",
    "\uD83D\uDC49 This usually creates 1 partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b254967-331b-4ecd-a836-47516ded7322",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "it_df = it_df.repartition(4)\n",
    "\n",
    "it_df.write.mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"/Volumes/learning/learning/learning/it_df_repartition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "461fde14-b5dc-4f9f-b3aa-c124b0b76e15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# lets check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5d5e7aa-d6a2-4c01-bbd8-730268b49f22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "it_df = it_df.coalesce(1)\n",
    "it_df.write.mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"/Volumes/learning/learning/learning/it_df_coalesce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc2dd3e5-2ef0-46d8-8ab5-b344d60eeedf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57c6415a-6946-404f-8714-995b0b6f8bf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+------+\n| id|   name|department|salary|\n+---+-------+----------+------+\n|  1|  Alice|        IT| 75000|\n|  2|    Bob|        HR| 65000|\n|  3|Charlie|   Finance| 80000|\n+---+-------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de83d66f-4822-491d-842f-10d04683d682",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#Create new rows as a DataFrame\n",
    "new_data = [\n",
    "    (4, \"David\", \"IT\", 72000),\n",
    "    (5, \"Eva\", \"HR\", 68000),\n",
    "    (6, \"Frank\", \"Finance\", 83000)\n",
    "]\n",
    "\n",
    "new_df = spark.createDataFrame(\n",
    "    new_data,\n",
    "    [\"id\", \"name\", \"department\", \"salary\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "772ebc17-2a12-409b-8c27-439811a2a2d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+------+\n| id|   name|department|salary|\n+---+-------+----------+------+\n|  1|  Alice|        IT| 75000|\n|  2|    Bob|        HR| 65000|\n|  3|Charlie|   Finance| 80000|\n|  4|  David|        IT| 72000|\n|  5|    Eva|        HR| 68000|\n|  6|  Frank|   Finance| 83000|\n+---+-------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "final_df = df1.union(new_df)\n",
    "final_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bee7a143-2bbb-4275-beec-399849f25585",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# WRITE MODE JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c22cd2eb-9712-4774-b8c3-3f463017a039",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .json(\"/Volumes/learning/learning/learning/total_employees.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "105e3666-2c64-443d-954f-416b0311fb79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# LETS SEE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19b0acbd-3bd4-4adb-9d98-0f4ab149d151",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df.coalesce(1) \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .json(\"/Volumes/learning/learning/learning/total_employees1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ae5b687-5386-4209-9067-e4348a0ee67c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# LETS SEE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5557cd58-3675-40af-a253-94fd4ccf6f0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d7041b6-66dd-47c1-8a2c-4d5b33c94545",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Parquet is a columnar file format designed for big data analytics (used heavily with Spark, Hive, Databricks, etc.).\n",
    "❌ Row-based storage (CSV, JSON)\n",
    "\n",
    "Data is stored row by row:\n",
    "\n",
    "1, Alice, IT, 75000\n",
    "2, Bob, HR, 65000\n",
    "3, Charlie, Finance, 80000\n",
    "\uD83D\uDC49 Reading only salary still scans entire rows\n",
    "\n",
    "✅ Column-based storage (Parquet)\n",
    "\n",
    "Data is stored column by column:\n",
    "\n",
    "id       → 1 | 2 | 3\n",
    "name     → Alice | Bob | Charlie\n",
    "dept     → IT | HR | Finance\n",
    "salary   → 75000 | 65000 | 80000\n",
    "\uD83D\uDC49 Reading only salary scans only that column\n",
    "\n",
    "1.Reads only required columns(faster Spark jobs)\n",
    "3.No need to infer schema every time\n",
    "4.Prevents schema mismatch errors\n",
    "5.Works perfectly with:\n",
    "SELECT salary\n",
    "GROUP BY department\n",
    "WHERE department = 'IT'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dcc0c9b0-28b3-470b-afd8-0c0ff0d87eae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# csv,json,parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54f39cf6-5fcc-45b9-9dd7-36099afe6c2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# csv \n",
    "id,name,salary\n",
    "1,Alice,5000\n",
    "2,Bob,6000\n",
    "\n",
    "# json\n",
    "{\n",
    "  \"id\": 1,\n",
    "  \"name\": \"Alice\",\n",
    "  \"address\": { \"city\": \"Delhi\", \"pin\": 110001 }\n",
    "}\n",
    "#parquet\n",
    "id:      [1,2,3]\n",
    "name:    [Alice,Bob,Carol]\n",
    "salary:  [5000,6000,7000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2c48a16-edbd-40eb-b302-124152dffdea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# DATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88a2dfb1-d6ca-406b-ab74-727de34f3ae4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+------+\n| id|   name|department|salary|\n+---+-------+----------+------+\n|  1|  Alice|        IT| 75000|\n|  2|    Bob|        HR| 65000|\n|  3|Charlie|   Finance| 80000|\n|  4|  Diana|        IT| 72000|\n+---+-------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (1, \"Alice\", \"IT\", 75000),\n",
    "    (2, \"Bob\", \"HR\", 65000),\n",
    "    (3, \"Charlie\", \"Finance\", 80000),\n",
    "    (4, \"Diana\", \"IT\", 72000)\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"name\", \"department\", \"salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed838a8b-2fb4-4079-ab62-a3010b23be0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# WRITE MODE PARQUET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94960b36-4479-46cb-9bd8-8ad1b4ee9722",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .parquet(\"/Volumes/learning/learning/learning/employees_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dcb66de6-ebaf-4128-91f1-19276fbb07f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# LETS SEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e0af85b-4198-42ae-9522-210df035b98e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.coalesce(1) \\\n",
    "  .write \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .parquet(\"/Volumes/learning/learning/learning/employees_parquet_single\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e23a4bd7-c9c0-4543-a6f4-1d59c42ea720",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "In Databricks,CSV, JSON, and Parquet is stored as a folder, not a single file.\n",
    "employees_parquet/\n",
    " ├── part-00000-8a9c.parquet\n",
    " ├── part-00001-8a9c.parquet\n",
    " ├── part-00002-8a9c.parquet\n",
    " ├── _SUCCESS\n",
    "Why multiple part-*.parquet fil\n",
    "\n",
    "Spark works in a distributed manner:\n",
    "\n",
    "1.Each partition of a DataFrame is written by a separate task\n",
    "2.Each task creates one file\n",
    "3.All files together form the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad1a0aa3-a0b9-4067-821f-011ffb49235e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# READ MODE PARQUET\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20ccadd3-8dd7-4306-ac03-016cda04c3c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+------+\n| id|   name|department|salary|\n+---+-------+----------+------+\n|  1|  Alice|        IT| 75000|\n|  2|    Bob|        HR| 65000|\n|  3|Charlie|   Finance| 80000|\n|  4|  Diana|        IT| 72000|\n+---+-------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "parquet_df = spark.read.parquet(\"/Volumes/learning/learning/learning/employees_parquet_single\")\n",
    "parquet_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78d36d23-92a9-49b3-89f6-87e12bfc4a8e",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1765816372404}",
       "filterBlob": "{\"version\":1,\"filterGroups\":[{\"enabled\":true,\"filterGroupId\":\"fg_744462fb\",\"op\":\"OR\",\"filters\":[{\"filterId\":\"f_296ccfab\",\"enabled\":true,\"columnId\":\"id\",\"dataType\":\"integer\",\"filterType\":\"oneof\"}],\"local\":false,\"updatedAt\":1765816360579}],\"syncTimestamp\":1765816417766}",
       "queryPlanFiltersBlob": "[]",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>department</th><th>salary</th></tr></thead><tbody><tr><td>1</td><td>Alice</td><td>IT</td><td>75000</td></tr><tr><td>2</td><td>Bob</td><td>HR</td><td>65000</td></tr><tr><td>3</td><td>Charlie</td><td>Finance</td><td>80000</td></tr><tr><td>4</td><td>Diana</td><td>IT</td><td>72000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Alice",
         "IT",
         75000
        ],
        [
         2,
         "Bob",
         "HR",
         65000
        ],
        [
         3,
         "Charlie",
         "Finance",
         80000
        ],
        [
         4,
         "Diana",
         "IT",
         72000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "department",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(parquet_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a4693c8-8015-4cc7-8288-9c6319a5c592",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Benefits of Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e19a35de-24eb-449a-836a-dbb2d2fd1921",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "1.Parquet stores data column-wise, not row-wise.\n",
    "\uD83D\uDC49 Spark reads only required columns, not the full table.\n",
    "eg.SELECT name, salary FROM employees;\n",
    "✔ Reads only name and salary columns\n",
    "❌ Skips other columns\n",
    "2.Smaller File Size (Compression)\n",
    "\uD83D\uDC49 70–90% size reduction compared to CSV/JSON\n",
    "3.Better Query Performance\n",
    "\uD83D\uDC49 Queries run much faster than CSV/JSON\n",
    "4.Schema Enforcement(no need to write inferschema=True)\n",
    "eg.✔ salary stays numeric\n",
    "❌ No accidental strings like \"five thousand\"\n",
    "5.splittable & Parallel Processing\n",
    "Files can be split\n",
    "Spark reads them in parallel\n",
    "\uD83D\uDC49 Better cluster utilization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e0e7c7e-7a81-4c22-b42c-15e3ec0199e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#updating the value and store in a parquet file again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f05a825f-5e63-4461-a252-871b15f2d181",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+------+\n| id|   name|department|salary|\n+---+-------+----------+------+\n|  1|  Alice|        IT| 75000|\n|  2|    Bob|        HR| 65000|\n|  3|Charlie|   Finance| 80000|\n|  4|  Diana|        IT| 72000|\n+---+-------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "385fec80-f151-4b31-8de8-ada49df27d03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+------+\n| id|   name|department|salary|\n+---+-------+----------+------+\n|  1|  Alice|   Finance| 75000|\n|  2|    Bob|        HR| 65000|\n|  3|Charlie|   Finance| 80000|\n|  4|  Diana|        IT| 72000|\n+---+-------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "#update the value in dept column where name =\"alice\"\n",
    "from pyspark.sql.functions import when, col\n",
    "updated_df = df.withColumn(\n",
    "    \"department\",\n",
    "    when(col(\"name\") == \"Alice\", \"Finance\")\n",
    "    .otherwise(col(\"department\"))\n",
    ")\n",
    "\n",
    "updated_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b797b84-352a-4e55-89b3-724b00ef2137",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Write the updated data back (Parquet / JSON)\n",
    "#Parquet (recommended)\n",
    "updated_df.write.mode(\"overwrite\").parquet(\"/Volumes/learning/learning/learning/employees_parquet_single\")\n",
    "\n",
    "#JSON\n",
    "#updated_df.write.mode(\"overwrite\").json(\"output/employees_json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ecce93f-9863-4372-b95f-421c01572f61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# LETS SEE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b33e6068-4c78-417e-ab4f-ae03d9e5fd79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# partition,repartition,coalesce\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00120604-7b02-496a-9cf8-69e6557403fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+------+\n| id|   name|department|salary|\n+---+-------+----------+------+\n|  1|  Alice|        IT| 75000|\n|  2|    Bob|        HR| 65000|\n|  3|Charlie|   Finance| 80000|\n|  4|  David|        IT| 72000|\n|  5|    Eva|        HR| 68000|\n|  6|  Frank|   Finance| 83000|\n|  7|  Grace|        IT| 77000|\n|  8|  Henry|        HR| 69000|\n+---+-------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (1, \"Alice\",   \"IT\",       75000),\n",
    "    (2, \"Bob\",     \"HR\",       65000),\n",
    "    (3, \"Charlie\", \"Finance\",  80000),\n",
    "    (4, \"David\",   \"IT\",       72000),\n",
    "    (5, \"Eva\",     \"HR\",       68000),\n",
    "    (6, \"Frank\",   \"Finance\",  83000),\n",
    "    (7, \"Grace\",   \"IT\",       77000),\n",
    "    (8, \"Henry\",   \"HR\",       69000)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    data,\n",
    "    [\"id\", \"name\", \"department\", \"salary\"]\n",
    ")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e725290d-801f-4413-b129-1bb602c90161",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").json(\n",
    "    \"/Volumes/learning/learning/learning/employees_no_repartition\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f378168-2759-4deb-9ae4-4a61d54b7a86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Result\n",
    "One file per partition\n",
    "Many small JSON files\n",
    "Not efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01b92a98-e52d-4d1f-b6e5-6c552839478a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_rep = df.repartition(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29304255-0f92-4311-a2e0-ea93e5a5304f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_rep.write.mode(\"overwrite\").json(\n",
    "    \"/Volumes/learning/learning/learning/employees_repartition\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24769507-9f18-4ebe-9957-6d025c8f78da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "What happens here\n",
    "Spark reshuffles data\n",
    "Evenly distributes rows\n",
    "2 output JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfacdb3b-995c-4a55-8837-217d14f1c814",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Use coalesce() (no shuffle – faster)\n",
    "df_coal = df.coalesce(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "096c1aa6-3d24-492f-9aeb-72c6c30c9221",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_coal.write.mode(\"overwrite\").json(\n",
    "    \"/Volumes/learning/learning/learning/employees_coalesce\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ce44d94-b9b8-4aa1-a6b4-71b78486ba78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "What happens\n",
    "No full shuffle\n",
    "Faster than repartition\n",
    "2 output files\n",
    "Possible uneven data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2c60bfe-1cad-41be-95da-ded4e51ab493",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Common operations that cause shuffle:\n",
    "\n",
    "repartition()\n",
    "\n",
    "groupBy()\n",
    "\n",
    "join()\n",
    "\n",
    "distinct()\n",
    "\n",
    "orderBy()\n",
    "\n",
    "reduceByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87d37a28-a6cf-4f0b-91f0-a1b5a268885f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "repartition(2) (WITH shuffle)\n",
    "What happens\n",
    "\n",
    "Spark reshuffles data\n",
    "Can increase OR decrease partitions\n",
    "Rows are evenly redistributed\n",
    "Result\n",
    "Partition 1 → Alice, Charlie, Eva, Grace\n",
    "Partition 2 → Bob, David, Frank, Henry\n",
    "\n",
    "Good for large datasets\n",
    "✔ Balanced partitions\n",
    "✔ Good parallelism\n",
    "❌ Expensive (shuffle)\n",
    "\n",
    "When to use\n",
    "\n",
    "Before writing large data\n",
    "To increase parallelism\n",
    "After heavy filtering / joins\n",
    "When data is skewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7f1a102-7ccb-4289-a522-1f5be0c9b0ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "coalesce(2) (NO shuffle)\n",
    "What happens\n",
    "\n",
    "Reduces number of partitions only\n",
    "It simply merges existing partitions\n",
    "After coalesce(2):\n",
    "\n",
    "Partition 1 → Alice, Bob, Charlie, David, Eva\n",
    "Partition 2 → Frank, Grace, Henry\n",
    "\n",
    "✔ No network cost\n",
    "❌ Uneven partitions possible\n",
    "✅ Very fast\n",
    "❌ Can cause data skew\n",
    "❌ Cannot increase partitions\n",
    "⚠️ Single executor load\n",
    "\n",
    "When to use\n",
    "Writing single output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8037353-b997-4614-aada-130cbe14b673",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "What does shuffle really mean? (Simple)\n",
    "\n",
    "Spark processes data in partitions.\n",
    "When Spark needs data in a new order or new grouping, it must:\n",
    "\n",
    "Break existing partitions\n",
    "\n",
    "Send data over the network\n",
    "\n",
    "Re-create new partitions\n",
    "\n",
    "\uD83D\uDC49 This whole process is called a shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "617de806-e326-4c44-9e6d-ed589da842df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# API'S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35f36be5-67a3-4b95-80dc-a23d6b5f68f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://jsonplaceholder.typicode.com/posts\"\n",
    "data = requests.get(url).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b846396c-220b-4769-bd91-3554b1262396",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---+--------------------------------------------------------------------------+------+\n|body                                                                                                                                                                                                                                |id |title                                                                     |userId|\n+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---+--------------------------------------------------------------------------+------+\n|quia et suscipit\\nsuscipit recusandae consequuntur expedita et cum\\nreprehenderit molestiae ut ut quas totam\\nnostrum rerum est autem sunt rem eveniet architecto                                                                   |1  |sunt aut facere repellat provident occaecati excepturi optio reprehenderit|1     |\n|est rerum tempore vitae\\nsequi sint nihil reprehenderit dolor beatae ea dolores neque\\nfugiat blanditiis voluptate porro vel nihil molestiae ut reiciendis\\nqui aperiam non debitis possimus qui neque nisi nulla                   |2  |qui est esse                                                              |1     |\n|et iusto sed quo iure\\nvoluptatem occaecati omnis eligendi aut ad\\nvoluptatem doloribus vel accusantium quis pariatur\\nmolestiae porro eius odio et labore et velit aut                                                             |3  |ea molestias quasi exercitationem repellat qui ipsa sit aut               |1     |\n|ullam et saepe reiciendis voluptatem adipisci\\nsit amet autem assumenda provident rerum culpa\\nquis hic commodi nesciunt rem tenetur doloremque ipsam iure\\nquis sunt voluptatem rerum illo velit                                   |4  |eum et est occaecati                                                      |1     |\n|repudiandae veniam quaerat sunt sed\\nalias aut fugiat sit autem sed est\\nvoluptatem omnis possimus esse voluptatibus quis\\nest aut tenetur dolor neque                                                                              |5  |nesciunt quas odio                                                        |1     |\n|ut aspernatur corporis harum nihil quis provident sequi\\nmollitia nobis aliquid molestiae\\nperspiciatis et ea nemo ab reprehenderit accusantium quas\\nvoluptate dolores velit et doloremque molestiae                               |6  |dolorem eum magni eos aperiam quia                                        |1     |\n|dolore placeat quibusdam ea quo vitae\\nmagni quis enim qui quis quo nemo aut saepe\\nquidem repellat excepturi ut quia\\nsunt ut sequi eos ea sed quas                                                                                |7  |magnam facilis autem                                                      |1     |\n|dignissimos aperiam dolorem qui eum\\nfacilis quibusdam animi sint suscipit qui sint possimus cum\\nquaerat magni maiores excepturi\\nipsam ut commodi dolor voluptatum modi aut vitae                                                 |8  |dolorem dolore est ipsam                                                  |1     |\n|consectetur animi nesciunt iure dolore\\nenim quia ad\\nveniam autem ut quam aut nobis\\net est aut quod aut provident voluptas autem voluptas                                                                                         |9  |nesciunt iure omnis dolorem tempora et accusantium                        |1     |\n|quo et expedita modi cum officia vel magni\\ndoloribus qui repudiandae\\nvero nisi sit\\nquos veniam quod sed accusamus veritatis error                                                                                                |10 |optio molestias id quia eum                                               |1     |\n|delectus reiciendis molestiae occaecati non minima eveniet qui voluptatibus\\naccusamus in eum beatae sit\\nvel qui neque voluptates ut commodi qui incidunt\\nut animi commodi                                                        |11 |et ea vero quia laudantium autem                                          |2     |\n|itaque id aut magnam\\npraesentium quia et ea odit et ea voluptas et\\nsapiente quia nihil amet occaecati quia id voluptatem\\nincidunt ea est distinctio odio                                                                         |12 |in quibusdam tempore odit est dolorem                                     |2     |\n|aut dicta possimus sint mollitia voluptas commodi quo doloremque\\niste corrupti reiciendis voluptatem eius rerum\\nsit cumque quod eligendi laborum minima\\nperferendis recusandae assumenda consectetur porro architecto ipsum ipsam|13 |dolorum ut in voluptas mollitia et saepe quo animi                        |2     |\n|fuga et accusamus dolorum perferendis illo voluptas\\nnon doloremque neque facere\\nad qui dolorum molestiae beatae\\nsed aut voluptas totam sit illum                                                                                 |14 |voluptatem eligendi optio                                                 |2     |\n|reprehenderit quos placeat\\nvelit minima officia dolores impedit repudiandae molestiae nam\\nvoluptas recusandae quis delectus\\nofficiis harum fugiat vitae                                                                          |15 |eveniet quod temporibus                                                   |2     |\n|suscipit nam nisi quo aperiam aut\\nasperiores eos fugit maiores voluptatibus quia\\nvoluptatem quis ullam qui in alias quia est\\nconsequatur magni mollitia accusamus ea nisi voluptate dicta                                        |16 |sint suscipit perspiciatis velit dolorum rerum ipsa laboriosam odio       |2     |\n|eos voluptas et aut odit natus earum\\naspernatur fuga molestiae ullam\\ndeserunt ratione qui eos\\nqui nihil ratione nemo velit ut aut id quo                                                                                         |17 |fugit voluptas sed molestias voluptatem provident                         |2     |\n|eveniet quo quis\\nlaborum totam consequatur non dolor\\nut et est repudiandae\\nest voluptatem vel debitis et magnam                                                                                                                  |18 |voluptate et itaque vero tempora molestiae                                |2     |\n|illum quis cupiditate provident sit magnam\\nea sed aut omnis\\nveniam maiores ullam consequatur atque\\nadipisci quo iste expedita sit quos voluptas                                                                                  |19 |adipisci placeat illum aut reiciendis qui                                 |2     |\n|qui consequuntur ducimus possimus quisquam amet similique\\nsuscipit porro ipsam amet\\neos veritatis officiis exercitationem vel fugit aut necessitatibus totam\\nomnis rerum consequatur expedita quidem cumque explicabo            |20 |doloribus ad provident suscipit at                                        |2     |\n+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---+--------------------------------------------------------------------------+------+\nonly showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(data)\n",
    "df.show(truncate=False) #Do NOT cut (shorten) long column values — show the full content.\n",
    "                                #Default width is 20 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7fefe3af-67ae-4c02-93af-8f6ea90ef0ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Column\tValue\n",
    "id\t1\n",
    "userId\t1\n",
    "title\t\"sunt aut facere repellat provident occaecati excepturi optio reprehenderit\"\n",
    "body\tA long text paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc915d70-13a6-47cc-aee8-aaaf74d19196",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- body: string (nullable = true)\n |-- id: long (nullable = true)\n |-- title: string (nullable = true)\n |-- userId: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e1a8a7c-9328-42f2-860a-b845ecdb62e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9552cc30-7714-4e1d-92ea-903b53eb1d46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#This is the JSON data you are sending to the server.(or add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cc24887-1ee7-40b7-b503-8e5f9ea7026f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201\n{'userId': 1, 'title': 'Learning PySpark API', 'body': 'This is a POST request example', 'id': 101}\n"
     ]
    }
   ],
   "source": [
    "#POST – Create data\n",
    "import requests\n",
    "\n",
    "post_data = {\n",
    "    \"userId\": 1,\n",
    "    \"title\": \"Learning PySpark API\",\n",
    "    \"body\": \"This is a POST request example\"\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    \"https://jsonplaceholder.typicode.com/posts\",\n",
    "    json=post_data\n",
    ")\n",
    "\n",
    "print(response.status_code)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "240250ab-a9e1-4ee0-ae1d-11bfe8febbcb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Because id :101,is created by the server, not by you\n",
    "When the server receives a POST request:\n",
    "\n",
    "It creates a new record\n",
    "\n",
    "It automatically generates a unique ID\n",
    "\n",
    "Then it sends the created object back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a8ef267-7452-4d16-99a0-92fb90d95047",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "What happens here:\n",
    "\n",
    "requests.post() → makes a POST request\n",
    "\n",
    "URL /posts → endpoint to create new data\n",
    "\n",
    "json=post_data:\n",
    "\n",
    "Converts Python dict → JSON\n",
    "\n",
    "Sends it in request body\n",
    "\n",
    "Sets Content-Type: application/json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de7f88ac-f912-4673-b31f-d5760da649a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# put - update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53beb9c4-d3a4-45ff-a822-f230a7a5480d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n{'userId': 1, 'title': 'Updated Title', 'body': 'This data is updated using PUT', 'id': 1}\n"
     ]
    }
   ],
   "source": [
    "#PUT – Update the SAME data\n",
    "\n",
    "#We update post id = 1\n",
    "\n",
    "update_data = {\n",
    "    \"userId\": 1,\n",
    "    \"title\": \"Updated Title\",\n",
    "    \"body\": \"This data is updated using PUT\"\n",
    "}\n",
    "\n",
    "response = requests.put(\n",
    "    \"https://jsonplaceholder.typicode.com/posts/1\",  # where userid =1\n",
    "    json=update_data\n",
    ")\n",
    "\n",
    "print(response.status_code)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e198d53-8f54-418c-92a3-49cd208b7609",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Real-world APIs (Critical concept)\n",
    "\n",
    "In real production APIs:\n",
    "\n",
    "id → ❌ never changeable\n",
    "\n",
    "userId → ❌ often NOT changeable (ownership field)\n",
    "\n",
    "title, body → ✅ changeable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ba2a63d-ee91-4d2e-a4e6-881c04e5e38d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "PUT vs PATCH (Interview favorite)\n",
    "PUT\n",
    "\n",
    "Replaces entire object\n",
    "\n",
    "Usually requires all fields\n",
    "\n",
    "PATCH\n",
    "\n",
    "Updates only specific fields\n",
    "\n",
    "requests.patch(\"/posts/1\", json={\"title\": \"New title\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71bcf741-5f7b-4ecc-b06f-a4ea80231ff3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n{'userId': 1, 'id': 1, 'title': 'New title', 'body': 'quia et suscipit\\nsuscipit recusandae consequuntur expedita et cum\\nreprehenderit molestiae ut ut quas totam\\nnostrum rerum est autem sunt rem eveniet architecto'}\n"
     ]
    }
   ],
   "source": [
    "response=requests.patch(\"https://jsonplaceholder.typicode.com/posts/1\", json={\"title\": \"New title\"})\n",
    "print(response.status_code)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cfcf882-92c1-46c2-a77b-3d6c9c013cec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n{}\n"
     ]
    }
   ],
   "source": [
    "#DELETE – Remove data\n",
    "import requests\n",
    "\n",
    "response = requests.delete(\n",
    "    \"https://jsonplaceholder.typicode.com/posts/1\"  # delete post with id = 1\n",
    ")\n",
    "\n",
    "print(response.status_code)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2b18760-5078-4450-82e6-41fa6f2b2ce2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "200 OK → deleted successfully (with response)\n",
    "201 Created – Resource successfully created (e.g., after POST).\n",
    "204 No Content → deleted successfully (no response body)\n",
    "404 Not Found – Requested resource doesn’t exist.\n",
    "503 Service Unavailable – Server temporarily unavailable (e.g., maintenance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80077951-136c-4388-9c6a-0fec0c9718c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# THANKU"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6074857923822551,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "pyspark_4 2025-12-13 21:16:23",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}